# -*- coding: utf-8 -*-
"""PerformanceEvaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QjU0Xxyac3BkMZIoeG05sS5a8OuEQy9a
"""

#  Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load the Dataset
import pandas as pd

# Replace 'path_to_your_file.csv' with the actual path to your file in Google Drive
file_path = '/content/drive/My Drive/Datasets/ASEAN_cleaned.csv'

# Load the dataset into a Pandas DataFrame
data = pd.read_csv(file_path)

# Display Dataset Info
print("Dataset Loaded Successfully!")
print("First 5 rows of the dataset:")
print(data.head())
print("\nDataset Info:")
print(data.info())

"""Feature Selection using Recursive Feature Elimination (RFE)"""

import numpy as np
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression

# Prepare the Dataset
# Drop rows with missing values
data = data.dropna()

# Define X (features) and y (target)
X = data.select_dtypes(include=[np.number]).drop(columns=["Food supply (kcal)"])
y = data["Food supply (kcal)"]

# Feature Selection using RFE
base_model = LinearRegression()  # Use Linear Regression as the base model for RFE
rfe_selector = RFE(estimator=base_model, n_features_to_select=4)  # Select top 4 features
rfe_selector.fit(X, y)

# Retrieve Selected Features
selected_features = X.columns[rfe_selector.support_]
print("Selected Features using RFE:", selected_features)

# Reduce the dataset to selected features
X_rfe = X[selected_features]

# Display the reduced feature set
print("\nReduced Dataset with Selected Features:")
print(X_rfe.head())

"""Random Forest Regressor (Holdout Validation)"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Holdout validation
X_train, X_temp, y_train, y_temp = train_test_split(X_rfe, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1/3, random_state=42)

# Train Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Evaluate on validation and test sets
y_val_pred = rf_model.predict(X_val)
y_test_pred = rf_model.predict(X_test)

val_metrics = {
    "MAE": mean_absolute_error(y_val, y_val_pred),
    "MSE": mean_squared_error(y_val, y_val_pred),
    "R²": r2_score(y_val, y_val_pred),
}
test_metrics = {
    "MAE": mean_absolute_error(y_test, y_test_pred),
    "MSE": mean_squared_error(y_test, y_test_pred),
    "R²": r2_score(y_test, y_test_pred),
}

print("Random Forest - Holdout Validation Metrics:")
print("Validation:", val_metrics)
print("Test:", test_metrics)

"""Random Forest Regressor (K-Fold Cross-Validation)"""

from sklearn.model_selection import KFold, cross_val_score

# K-Fold Cross Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(rf_model, X_rfe, y, scoring='neg_mean_squared_error', cv=kf)

print("Random Forest - K-Fold Cross-Validation Metrics:")
print("Mean MSE:", -np.mean(cv_scores))
print("Standard Deviation of MSE:", np.std(cv_scores))

"""Linear Regression (Holdout Validation)"""

# Train Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Evaluate on validation and test sets
y_val_pred = lr_model.predict(X_val)
y_test_pred = lr_model.predict(X_test)

val_metrics = {
    "MAE": mean_absolute_error(y_val, y_val_pred),
    "MSE": mean_squared_error(y_val, y_val_pred),
    "R²": r2_score(y_val, y_val_pred),
}
test_metrics = {
    "MAE": mean_absolute_error(y_test, y_test_pred),
    "MSE": mean_squared_error(y_test, y_test_pred),
    "R²": r2_score(y_test, y_test_pred),
}

print("Linear Regression - Holdout Validation Metrics:")
print("Validation:", val_metrics)
print("Test:", test_metrics)

"""Linear Regression (K-Fold Cross-Validation)"""

# K-Fold Cross Validation
cv_scores = cross_val_score(lr_model, X_rfe, y, scoring='neg_mean_squared_error', cv=kf)

print("Linear Regression - K-Fold Cross-Validation Metrics:")
print("Mean MSE:", -np.mean(cv_scores))
print("Standard Deviation of MSE:", np.std(cv_scores))

"""K-Nearest Neighbors Regressor (Holdout Validation)"""

from sklearn.neighbors import KNeighborsRegressor

# Train KNN model
knn_model = KNeighborsRegressor(n_neighbors=5)
knn_model.fit(X_train, y_train)

# Evaluate on validation and test sets
y_val_pred = knn_model.predict(X_val)
y_test_pred = knn_model.predict(X_test)

val_metrics = {
    "MAE": mean_absolute_error(y_val, y_val_pred),
    "MSE": mean_squared_error(y_val, y_val_pred),
    "R²": r2_score(y_val, y_val_pred),
}
test_metrics = {
    "MAE": mean_absolute_error(y_test, y_test_pred),
    "MSE": mean_squared_error(y_test, y_test_pred),
    "R²": r2_score(y_test, y_test_pred),
}

print("K-Nearest Neighbors - Holdout Validation Metrics:")
print("Validation:", val_metrics)
print("Test:", test_metrics)

"""K-Nearest Neighbors Regressor (K-Fold Cross-Validation)"""

# K-Fold Cross Validation
cv_scores = cross_val_score(knn_model, X_rfe, y, scoring='neg_mean_squared_error', cv=kf)

print("K-Nearest Neighbors - K-Fold Cross-Validation Metrics:")
print("Mean MSE:", -np.mean(cv_scores))
print("Standard Deviation of MSE:", np.std(cv_scores))

"""Support Vector Machine Regressor (Holdout Validation)"""

from sklearn.svm import SVR

# Train SVM model
svm_model = SVR(kernel='rbf', C=1.0, gamma=0.1)
svm_model.fit(X_train, y_train)

# Evaluate on validation and test sets
y_val_pred = svm_model.predict(X_val)
y_test_pred = svm_model.predict(X_test)

val_metrics = {
    "MAE": mean_absolute_error(y_val, y_val_pred),
    "MSE": mean_squared_error(y_val, y_val_pred),
    "R²": r2_score(y_val, y_val_pred),
}
test_metrics = {
    "MAE": mean_absolute_error(y_test, y_test_pred),
    "MSE": mean_squared_error(y_test, y_test_pred),
    "R²": r2_score(y_test, y_test_pred),
}

print("SVM - Holdout Validation Metrics:")
print("Validation:", val_metrics)
print("Test:", test_metrics)

"""Support Vector Machine Regressor (K-Fold Cross-Validation)"""

# K-Fold Cross Validation
cv_scores = cross_val_score(svm_model, X_rfe, y, scoring='neg_mean_squared_error', cv=kf)

print("SVM - K-Fold Cross-Validation Metrics:")
print("Mean MSE:", -np.mean(cv_scores))
print("Standard Deviation of MSE:", np.std(cv_scores))

"""Performance Metric Evaluation and Visualization"""

import matplotlib.pyplot as plt
import pandas as pd

# Function to summarize performance metrics
def summarize_metrics(algorithm, holdout_metrics, kfold_metrics):
    return {
        "Algorithm": algorithm,
        "Holdout MAE": holdout_metrics["MAE"],
        "Holdout MSE": holdout_metrics["MSE"],
        "Holdout R²": holdout_metrics["R²"],
        "K-Fold Mean MSE": kfold_metrics["mean_mse"],
        "K-Fold Std MSE": kfold_metrics["std_mse"]
    }

# Store metrics for each algorithm
metrics_summary = []

# Random Forest
rf_holdout_metrics = test_metrics  # From Random Forest holdout evaluation
rf_kfold_metrics = {"mean_mse": -np.mean(cv_scores), "std_mse": np.std(cv_scores)}  # From K-Fold
metrics_summary.append(summarize_metrics("Random Forest", rf_holdout_metrics, rf_kfold_metrics))

# Linear Regression
lr_holdout_metrics = test_metrics  # From Linear Regression holdout evaluation
cv_scores_lr = cross_val_score(lr_model, X_rfe, y, scoring='neg_mean_squared_error', cv=kf)  # Recompute for clarity
lr_kfold_metrics = {"mean_mse": -np.mean(cv_scores_lr), "std_mse": np.std(cv_scores_lr)}
metrics_summary.append(summarize_metrics("Linear Regression", lr_holdout_metrics, lr_kfold_metrics))

# KNN
knn_holdout_metrics = test_metrics  # From KNN holdout evaluation
cv_scores_knn = cross_val_score(knn_model, X_rfe, y, scoring='neg_mean_squared_error', cv=kf)  # Recompute for clarity
knn_kfold_metrics = {"mean_mse": -np.mean(cv_scores_knn), "std_mse": np.std(cv_scores_knn)}
metrics_summary.append(summarize_metrics("KNN", knn_holdout_metrics, knn_kfold_metrics))

# SVM
svm_holdout_metrics = test_metrics  # From SVM holdout evaluation
cv_scores_svm = cross_val_score(svm_model, X_rfe, y, scoring='neg_mean_squared_error', cv=kf)  # Recompute for clarity
svm_kfold_metrics = {"mean_mse": -np.mean(cv_scores_svm), "std_mse": np.std(cv_scores_svm)}
metrics_summary.append(summarize_metrics("SVM", svm_holdout_metrics, svm_kfold_metrics))

# Create a DataFrame for comparison
metrics_df = pd.DataFrame(metrics_summary)

# Display the metrics
print("\nPerformance Comparison Between Holdout Validation and K-Fold Cross-Validation:")
print(metrics_df)

# Visualization: Bar Chart for MAE and R²
plt.figure(figsize=(12, 6))
# MAE
plt.subplot(1, 2, 1)
metrics_df.plot(x="Algorithm", y=["Holdout MAE"], kind="bar", legend=False, ax=plt.gca(), color=["skyblue"])
plt.title("MAE Comparison")
plt.ylabel("MAE")
plt.xlabel("Algorithm")
plt.xticks(rotation=45)
plt.grid(axis="y")

# R²
plt.subplot(1, 2, 2)
metrics_df.plot(x="Algorithm", y=["Holdout R²"], kind="bar", legend=False, ax=plt.gca(), color=["lightgreen"])
plt.title("R² Comparison")
plt.ylabel("R² Score")
plt.xlabel("Algorithm")
plt.xticks(rotation=45)
plt.grid(axis="y")

plt.tight_layout()
plt.show()

# Visualization: Learning Curves for Random Forest (as an example)
from sklearn.model_selection import learning_curve

def plot_learning_curve(estimator, X, y, cv, title):
    train_sizes, train_scores, val_scores = learning_curve(estimator, X, y, cv=cv, scoring='neg_mean_squared_error')
    train_scores_mean = -train_scores.mean(axis=1)
    val_scores_mean = -val_scores.mean(axis=1)

    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_scores_mean, label="Training Error", marker="o")
    plt.plot(train_sizes, val_scores_mean, label="Validation Error", marker="o")
    plt.title(title)
    plt.xlabel("Training Size")
    plt.ylabel("Mean Squared Error")
    plt.legend()
    plt.grid()
    plt.show()

# Plot learning curve for Random Forest
plot_learning_curve(rf_model, X_rfe, y, cv=kf, title="Learning Curve - Random Forest")